#!/usr/bin/env python3
"""
Setup script for Ollama with vision model for StorySign
"""

import asyncio
import aiohttp
import subprocess
import sys
import time
import json

async def check_ollama_running():
    """Check if Ollama is running"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get("http://localhost:11434/api/tags", timeout=aiohttp.ClientTimeout(total=5)) as response:
                return response.status == 200
    except:
        return False

async def check_model_available(model_name="llava:7b"):
    """Check if the vision model is available"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get("http://localhost:11434/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    models = [model.get('name', '') for model in data.get('models', [])]
                    return any(model_name in model for model in models)
    except:
        pass
    return False

def install_ollama():
    """Install Ollama using the official installer"""
    print("üì¶ Installing Ollama...")
    try:
        # For macOS/Linux
        result = subprocess.run([
            "curl", "-fsSL", "https://ollama.com/install.sh"
        ], capture_output=True, text=True, check=True)
        
        # Run the installer
        subprocess.run(["sh"], input=result.stdout, text=True, check=True)
        print("‚úÖ Ollama installed successfully!")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Failed to install Ollama: {e}")
        return False

def start_ollama():
    """Start Ollama service"""
    print("üöÄ Starting Ollama service...")
    try:
        # Start Ollama in the background
        subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        # Wait a bit for it to start
        time.sleep(3)
        return True
    except Exception as e:
        print(f"‚ùå Failed to start Ollama: {e}")
        return False

def pull_vision_model(model_name="llava:7b"):
    """Pull the vision model"""
    print(f"üì• Pulling vision model: {model_name}")
    print("‚è≥ This may take several minutes for the first time...")
    
    try:
        result = subprocess.run(
            ["ollama", "pull", model_name],
            capture_output=True,
            text=True,
            timeout=600  # 10 minute timeout
        )
        
        if result.returncode == 0:
            print(f"‚úÖ Model {model_name} pulled successfully!")
            return True
        else:
            print(f"‚ùå Failed to pull model: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("‚ùå Model pull timed out (10 minutes)")
        return False
    except Exception as e:
        print(f"‚ùå Error pulling model: {e}")
        return False

async def test_vision_model(model_name="llava:7b"):
    """Test the vision model with a simple image"""
    print(f"üß™ Testing vision model: {model_name}")
    
    # Simple test image (1x1 red pixel)
    test_image_b64 = "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg=="
    
    payload = {
        "model": model_name,
        "prompt": "What do you see in this image? Respond with just the main object or color.",
        "images": [test_image_b64],
        "stream": False,
        "options": {
            "temperature": 0.1,
            "num_predict": 20
        }
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    response_text = data.get('response', '').strip()
                    print(f"‚úÖ Vision model test successful!")
                    print(f"   Response: {response_text}")
                    return True
                else:
                    print(f"‚ùå Vision model test failed: {response.status}")
                    return False
    except Exception as e:
        print(f"‚ùå Vision model test error: {e}")
        return False

async def main():
    """Main setup function"""
    print("üîß StorySign Ollama Vision Setup")
    print("=" * 50)
    
    # Check if Ollama is already running
    print("1Ô∏è‚É£ Checking if Ollama is running...")
    if await check_ollama_running():
        print("‚úÖ Ollama is already running!")
    else:
        print("‚ùå Ollama is not running")
        
        # Try to start it
        if start_ollama():
            # Wait and check again
            await asyncio.sleep(2)
            if await check_ollama_running():
                print("‚úÖ Ollama started successfully!")
            else:
                print("‚ùå Ollama failed to start. You may need to install it first.")
                print("üí° Run: curl -fsSL https://ollama.com/install.sh | sh")
                return False
        else:
            print("‚ùå Could not start Ollama. Please install it manually:")
            print("üí° Visit: https://ollama.com/download")
            return False
    
    # Check if vision model is available
    print("\n2Ô∏è‚É£ Checking if vision model is available...")
    model_name = "llava:7b"
    if await check_model_available(model_name):
        print(f"‚úÖ Model {model_name} is already available!")
    else:
        print(f"‚ùå Model {model_name} not found")
        
        # Pull the model
        if pull_vision_model(model_name):
            print(f"‚úÖ Model {model_name} installed successfully!")
        else:
            print(f"‚ùå Failed to install model {model_name}")
            return False
    
    # Test the vision model
    print("\n3Ô∏è‚É£ Testing vision model...")
    if await test_vision_model(model_name):
        print("‚úÖ Vision model is working correctly!")
    else:
        print("‚ùå Vision model test failed")
        return False
    
    print("\nüéâ Setup completed successfully!")
    print("=" * 50)
    print("‚úÖ Ollama is running with vision model")
    print("‚úÖ StorySign backend is configured to use Ollama")
    print("‚úÖ Groq API is configured for story generation")
    print("\nüöÄ You can now start the StorySign backend:")
    print("   python main_api.py")
    
    return True

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)