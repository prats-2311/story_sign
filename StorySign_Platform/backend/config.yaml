# StorySign ASL Platform Configuration
# This file contains configuration settings for the backend application

# Video capture and processing settings
video:
  width: 320 # Video frame width in pixels (320-1920)
  height: 240 # Video frame height in pixels (240-1080)
  fps: 30 # Target frames per second (10-60)
  format: "JPEG" # Video format codec (MJPG, YUYV, H264)
  quality: 70 # JPEG compression quality (optimized for low latency)

# MediaPipe Holistic model settings
mediapipe:
  min_detection_confidence: 0.5 # Minimum confidence for person detection (0.0-1.0)
  min_tracking_confidence: 0.5 # Minimum confidence for landmark tracking (0.0-1.0)
  model_complexity: 0 # Model complexity: 0=lite/fastest, 1=full, 2=heavy (optimized for speed)
  enable_segmentation: false # Enable pose segmentation mask
  refine_face_landmarks: true # Enable refined face landmark detection

# Server configuration settings
server:
  host: "0.0.0.0" # Server host address
  port: 8000 # Server port number (1024-65535)
  reload: true # Enable auto-reload in development
  log_level: "info" # Logging level (debug, info, warning, error, critical)
  max_connections: 10 # Maximum WebSocket connections (1-100)

# Local vision service configuration
local_vision:
  service_url: "http://localhost:11434" # Ollama service URL (default port)
  model_name: "meta-llama/llama-4-scout-17b-16e-instruct" # Groq Llama 4 Scout vision model
  service_type: "groq" # Service type: "ollama", "lm_studio", or "groq"
  timeout_seconds: 30 # Request timeout in seconds
  max_retries: 3 # Maximum retry attempts
  enabled: true # Enable/disable local vision service

# Groq API configuration (now for vision)
groq:
  api_key: "your_groq_api_key_here" # Groq API key (use environment variable in production)
  base_url: "https://api.groq.com/openai/v1" # Groq API base URL
  model_name: "meta-llama/llama-4-scout-17b-16e-instruct" # Groq Llama 4 Scout vision model
  timeout_seconds: 30 # Request timeout in seconds
  max_retries: 3 # Maximum retry attempts
  max_tokens: 100 # Maximum tokens for object identification
  temperature: 0.1 # Low temperature for consistent object identification
  enabled: true # Enable Groq API service

# Ollama LLM service configuration
# backend/config.yaml

ollama_config:
  # Use the official Ollama Cloud API endpoint
  service_url: "https://ollama.com"

  # Specify the correct cloud model you want to use
  story_model: "gpt-oss:20b"
  analysis_model: "gpt-oss:20b"

  # Add your API key here (keep it secret)
  api_key: "f945208978394c218ef31ed075c6c232.ap5HadPbSpSHwURc6anBVAbz"

  # Keep timeouts as they are, or adjust if needed
  timeout_seconds: 60
  max_tokens: 1500

# TiDB Database configuration
database:
  # Connection settings
  host: "gateway01.ap-southeast-1.prod.aws.tidbcloud.com" # TiDB Cloud server host
  port: 4000 # TiDB server port (default: 4000)
  database: "test" # Database name (from TiDB Cloud screenshot)
  username: "28XbMEz3PD5h7d6.root" # Database username (from TiDB Cloud connection string)
  password: "ek1FdpnVe3LDPcns" # Database password from TiDB Cloud

  # Connection pool settings
  pool_size: 10 # Connection pool size (1-100)
  max_overflow: 20 # Maximum pool overflow (0-100)
  pool_timeout: 30 # Pool timeout in seconds (5-300)
  pool_recycle: 3600 # Pool recycle time in seconds (300-86400)

  # SSL settings (enabled for TiDB Cloud)
  ssl_disabled: false # Enable SSL for TiDB Cloud
  ssl_ca: null # SSL CA certificate path (TiDB Cloud handles this)
  ssl_cert: null # SSL certificate path
  ssl_key: null # SSL private key path

  # Query settings
  query_timeout: 30 # Query timeout in seconds (5-300)
  echo_queries: true # Echo SQL queries for debugging (set to false in production)

  # Health check settings
  health_check_interval: 30 # Health check interval in seconds (10-300)
  max_retries: 3 # Maximum connection retry attempts (1-10)
  retry_delay: 1.0 # Delay between retries in seconds (0.1-10.0)
